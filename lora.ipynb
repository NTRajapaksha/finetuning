{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "14d12a3681ec492f8b12085d61be920d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_93ef00eeab8640a488774c3959c35a93",
              "IPY_MODEL_d95eafa0a09541bb8485675718ef4c53",
              "IPY_MODEL_298e8fd84b964ef1afa139ec39b63d3c"
            ],
            "layout": "IPY_MODEL_4c3b1d1f585945459e8f3dc4218e541e"
          }
        },
        "93ef00eeab8640a488774c3959c35a93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c64eb8e48e74441b9cd0f6520a4e426c",
            "placeholder": "​",
            "style": "IPY_MODEL_7bf0bf39547e4d4da98daf0acc384092",
            "value": "Map: 100%"
          }
        },
        "d95eafa0a09541bb8485675718ef4c53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8482d03d68843e483ef08d976c1a85a",
            "max": 5000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a1c3c04c0fb04803a705d26af9890aed",
            "value": 5000
          }
        },
        "298e8fd84b964ef1afa139ec39b63d3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_731ff93e131848578853e9f11a82f6ec",
            "placeholder": "​",
            "style": "IPY_MODEL_f8c31d9baceb40fc8cf53f66958745ab",
            "value": " 5000/5000 [00:07&lt;00:00, 703.25 examples/s]"
          }
        },
        "4c3b1d1f585945459e8f3dc4218e541e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c64eb8e48e74441b9cd0f6520a4e426c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bf0bf39547e4d4da98daf0acc384092": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8482d03d68843e483ef08d976c1a85a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1c3c04c0fb04803a705d26af9890aed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "731ff93e131848578853e9f11a82f6ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8c31d9baceb40fc8cf53f66958745ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "584f0589e1c74e2a8767d89fc4774567": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc99875c6558471f81354f2bdb695514",
              "IPY_MODEL_ac0cdbddda304fe99b5df94993f7ba7d",
              "IPY_MODEL_b4d3adceea4144c4bc83c0d2d932ade3"
            ],
            "layout": "IPY_MODEL_fa4cddc238bd4b7e8209862c34a8f0d8"
          }
        },
        "dc99875c6558471f81354f2bdb695514": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9d62867ead74bd5b7ce20b817580b70",
            "placeholder": "​",
            "style": "IPY_MODEL_5ecab5d11182498f8faac156b82d0937",
            "value": "Map: 100%"
          }
        },
        "ac0cdbddda304fe99b5df94993f7ba7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00757e2db0d246e095f5f43cebe27036",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce7c553cb7694bfa89d958453a5feef3",
            "value": 1000
          }
        },
        "b4d3adceea4144c4bc83c0d2d932ade3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96f83733a8a740aaa99901782e8699bd",
            "placeholder": "​",
            "style": "IPY_MODEL_862bca04884a4e568de7989423c4f526",
            "value": " 1000/1000 [00:01&lt;00:00, 721.85 examples/s]"
          }
        },
        "fa4cddc238bd4b7e8209862c34a8f0d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9d62867ead74bd5b7ce20b817580b70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ecab5d11182498f8faac156b82d0937": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00757e2db0d246e095f5f43cebe27036": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce7c553cb7694bfa89d958453a5feef3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "96f83733a8a740aaa99901782e8699bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "862bca04884a4e568de7989423c4f526": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setup with PEFT"
      ],
      "metadata": {
        "id": "wPcvcvqGVdBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install advanced fine-tuning libraries\n",
        "# !pip install transformers datasets peft accelerate evaluate rouge-score -q\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    default_data_collator\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU only'}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHPh3awjVXx_",
        "outputId": "020b4c13-9375-4074-b84a-1e561a38fa68"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Dataset Loading and Preprocessing"
      ],
      "metadata": {
        "id": "XwS_18O5VhZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SQuAD 2.0 dataset - includes unanswerable questions\n",
        "print(\"Loading SQuAD 2.0 dataset...\")\n",
        "dataset = load_dataset(\"squad_v2\")\n",
        "\n",
        "# Create smaller subset for efficient training\n",
        "def create_qa_subset(dataset, train_size=5000, val_size=1000):\n",
        "    \"\"\"Create balanced subset with answerable and unanswerable questions\"\"\"\n",
        "    # Shuffle and select based on indices\n",
        "    train_indices = list(range(len(dataset['train'])))\n",
        "    np.random.seed(42)\n",
        "    np.random.shuffle(train_indices)\n",
        "    train_indices = train_indices[:train_size]\n",
        "\n",
        "    val_indices = list(range(len(dataset['validation'])))\n",
        "    np.random.seed(42)\n",
        "    np.random.shuffle(val_indices)\n",
        "    val_indices = val_indices[:val_size]\n",
        "\n",
        "    train_dataset = dataset['train'].select(train_indices)\n",
        "    val_dataset = dataset['validation'].select(val_indices)\n",
        "\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "train_dataset, val_dataset = create_qa_subset(dataset)\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "\n",
        "# Analyze dataset complexity\n",
        "answerable_train = sum(1 for ex in train_dataset if len(ex['answers']['text']) > 0)\n",
        "print(f\"Answerable questions in training: {answerable_train} ({answerable_train/len(train_dataset)*100:.1f}%)\")\n",
        "\n",
        "# Display sample data\n",
        "sample = train_dataset[0]\n",
        "print(f\"\\nSample Question: {sample['question']}\")\n",
        "print(f\"Context (first 200 chars): {sample['context'][:200]}...\")\n",
        "print(f\"Answer: {sample['answers']['text'][0] if sample['answers']['text'] else 'No answer'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRF6asGgVf_4",
        "outputId": "863728bd-f67f-415f-aa2d-add17528c98b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading SQuAD 2.0 dataset...\n",
            "Training samples: 5000\n",
            "Validation samples: 1000\n",
            "Answerable questions in training: 3379 (67.6%)\n",
            "\n",
            "Sample Question: What year did the global recession that followed the financial crisis of 2007 end?\n",
            "Context (first 200 chars): It threatened the collapse of large financial institutions, which was prevented by the bailout of banks by national governments, but stock markets still dropped worldwide. In many areas, the housing m...\n",
            "Answer: 2012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Tokenization for QA"
      ],
      "metadata": {
        "id": "4jEsfgH2VfAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def preprocess_qa_data(examples):\n",
        "    \"\"\"Advanced preprocessing for question-answering tasks\"\"\"\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        sample_idx = sample_map[i]\n",
        "        answer = answers[sample_idx]\n",
        "        start_char = answer[\"answer_start\"][0] if answer[\"answer_start\"] else 0\n",
        "        end_char = start_char + len(answer[\"text\"][0]) if answer[\"text\"] else 0\n",
        "\n",
        "        # Find token positions\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "        context_start = sequence_ids.index(1)\n",
        "        context_end = len(sequence_ids) - 1 - sequence_ids[::-1].index(1)\n",
        "\n",
        "        # Handle unanswerable questions\n",
        "        if not answer[\"text\"]:\n",
        "            start_positions.append(context_start)\n",
        "            end_positions.append(context_start)\n",
        "        else:\n",
        "            # Find start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs\n",
        "\n",
        "# Apply preprocessing\n",
        "print(\"Preprocessing datasets...\")\n",
        "train_dataset = train_dataset.map(\n",
        "    preprocess_qa_data,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names\n",
        ")\n",
        "\n",
        "val_dataset = val_dataset.map(\n",
        "    preprocess_qa_data,\n",
        "    batched=True,\n",
        "    remove_columns=val_dataset.column_names\n",
        ")\n",
        "\n",
        "print(\"Preprocessing completed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "14d12a3681ec492f8b12085d61be920d",
            "93ef00eeab8640a488774c3959c35a93",
            "d95eafa0a09541bb8485675718ef4c53",
            "298e8fd84b964ef1afa139ec39b63d3c",
            "4c3b1d1f585945459e8f3dc4218e541e",
            "c64eb8e48e74441b9cd0f6520a4e426c",
            "7bf0bf39547e4d4da98daf0acc384092",
            "f8482d03d68843e483ef08d976c1a85a",
            "a1c3c04c0fb04803a705d26af9890aed",
            "731ff93e131848578853e9f11a82f6ec",
            "f8c31d9baceb40fc8cf53f66958745ab",
            "584f0589e1c74e2a8767d89fc4774567",
            "dc99875c6558471f81354f2bdb695514",
            "ac0cdbddda304fe99b5df94993f7ba7d",
            "b4d3adceea4144c4bc83c0d2d932ade3",
            "fa4cddc238bd4b7e8209862c34a8f0d8",
            "c9d62867ead74bd5b7ce20b817580b70",
            "5ecab5d11182498f8faac156b82d0937",
            "00757e2db0d246e095f5f43cebe27036",
            "ce7c553cb7694bfa89d958453a5feef3",
            "96f83733a8a740aaa99901782e8699bd",
            "862bca04884a4e568de7989423c4f526"
          ]
        },
        "id": "Sh0zU6tdVnks",
        "outputId": "9ec72061-3b00-4523-cda8-65d3c497a6d3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing datasets...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14d12a3681ec492f8b12085d61be920d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "584f0589e1c74e2a8767d89fc4774567"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LoRA Configuration and Model Setup"
      ],
      "metadata": {
        "id": "IKo78ieQVshZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load base model\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "# Configure LoRA - this is the key innovation\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.QUESTION_ANS,\n",
        "    inference_mode=False,\n",
        "    r=16,                    # Rank - higher = more parameters but better performance\n",
        "    lora_alpha=32,           # Scaling parameter\n",
        "    lora_dropout=0.1,        # Dropout for regularization\n",
        "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"]  # DistilBERT specific\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Move to GPU\n",
        "model.to(device)\n",
        "\n",
        "print(\"LoRA model configuration:\")\n",
        "print(f\"Base model parameters: {sum(p.numel() for p in model.base_model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2myYHd1RVsXU",
        "outputId": "fdcc2eb5-44a7-45ab-d4d7-5561f2376d95"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 591,362 || all params: 66,955,780 || trainable%: 0.8832127711752443\n",
            "LoRA model configuration:\n",
            "Base model parameters: 66,955,780\n",
            "Trainable parameters: 591,362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Training Configuration"
      ],
      "metadata": {
        "id": "75BhFKc9Vx6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments optimized for LoRA\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qa_lora_results\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    logging_steps=50,\n",
        "    save_steps=200,\n",
        "    learning_rate=3e-4,          # Higher LR for LoRA\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=100,\n",
        "    fp16=True,\n",
        "    dataloader_num_workers=2,\n",
        "    remove_unused_columns=False,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Custom metrics for QA evaluation\n",
        "def compute_qa_metrics(eval_pred):\n",
        "    \"\"\"Compute F1 and Exact Match scores for QA\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    start_predictions, end_predictions = predictions\n",
        "    start_labels, end_labels = labels\n",
        "\n",
        "    # Simplified evaluation - exact match for start/end positions\n",
        "    start_accuracy = (start_predictions.argmax(-1) == start_labels).mean()\n",
        "    end_accuracy = (end_predictions.argmax(-1) == end_labels).mean()\n",
        "    exact_match = ((start_predictions.argmax(-1) == start_labels) &\n",
        "                   (end_predictions.argmax(-1) == end_labels)).mean()\n",
        "\n",
        "    return {\n",
        "        \"start_accuracy\": start_accuracy,\n",
        "        \"end_accuracy\": end_accuracy,\n",
        "        \"exact_match\": exact_match\n",
        "    }\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=default_data_collator,\n",
        "    compute_metrics=compute_qa_metrics,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(\"Training configuration complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnwLcpNBVxu-",
        "outputId": "f9d517f1-0c97-47ea-d920-fe85c3a51d6c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training configuration complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LoRA Training Process"
      ],
      "metadata": {
        "id": "SXMAQ794V19h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "print(\"Starting LoRA fine-tuning for Question Answering...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\nTraining completed in {training_time/60:.2f} minutes\")\n",
        "\n",
        "# Save LoRA adapter\n",
        "model.save_pretrained(\"./qa_lora_adapter\")\n",
        "print(\"LoRA adapter saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "9gceSxZWV12N",
        "outputId": "f844b9ca-5126-4b3c-850c-b91c1b445322"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting LoRA fine-tuning for Question Answering...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1896' max='1896' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1896/1896 04:18, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Start Accuracy</th>\n",
              "      <th>End Accuracy</th>\n",
              "      <th>Exact Match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.432700</td>\n",
              "      <td>2.795097</td>\n",
              "      <td>0.547783</td>\n",
              "      <td>0.521182</td>\n",
              "      <td>0.515271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.969000</td>\n",
              "      <td>2.274730</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>0.549754</td>\n",
              "      <td>0.543842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>2.642900</td>\n",
              "      <td>2.103418</td>\n",
              "      <td>0.458128</td>\n",
              "      <td>0.430542</td>\n",
              "      <td>0.328079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>2.199600</td>\n",
              "      <td>1.892962</td>\n",
              "      <td>0.457143</td>\n",
              "      <td>0.465025</td>\n",
              "      <td>0.352709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.998800</td>\n",
              "      <td>1.849060</td>\n",
              "      <td>0.498522</td>\n",
              "      <td>0.430542</td>\n",
              "      <td>0.335961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>2.008300</td>\n",
              "      <td>1.748764</td>\n",
              "      <td>0.497537</td>\n",
              "      <td>0.490640</td>\n",
              "      <td>0.373399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.906600</td>\n",
              "      <td>1.780151</td>\n",
              "      <td>0.477833</td>\n",
              "      <td>0.473892</td>\n",
              "      <td>0.361576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>1.965800</td>\n",
              "      <td>1.728884</td>\n",
              "      <td>0.478818</td>\n",
              "      <td>0.478818</td>\n",
              "      <td>0.348768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>1.728200</td>\n",
              "      <td>1.675193</td>\n",
              "      <td>0.504433</td>\n",
              "      <td>0.497537</td>\n",
              "      <td>0.387192</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training completed in 4.34 minutes\n",
            "LoRA adapter saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Evaluation and Testing"
      ],
      "metadata": {
        "id": "Bob5bXtmV6rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comprehensive evaluation\n",
        "print(\"Evaluating LoRA-tuned model...\")\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(\"\\nFinal Evaluation Results:\")\n",
        "print(\"-\" * 40)\n",
        "for key, value in eval_results.items():\n",
        "    if key.startswith('eval_'):\n",
        "        metric_name = key.replace('eval_', '').title()\n",
        "        print(f\"{metric_name}: {value:.4f}\")\n",
        "\n",
        "# Advanced inference function\n",
        "def answer_question(question, context, model, tokenizer):\n",
        "    \"\"\"Extract answer from context using fine-tuned QA model\"\"\"\n",
        "    inputs = tokenizer(\n",
        "        question,\n",
        "        context,\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    ).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        start_scores = outputs.start_logits\n",
        "        end_scores = outputs.end_logits\n",
        "\n",
        "        start_idx = torch.argmax(start_scores)\n",
        "        end_idx = torch.argmax(end_scores)\n",
        "\n",
        "        # Get confidence scores\n",
        "        start_confidence = torch.softmax(start_scores, dim=-1)[0][start_idx].item()\n",
        "        end_confidence = torch.softmax(end_scores, dim=-1)[0][end_idx].item()\n",
        "\n",
        "        if start_idx <= end_idx:\n",
        "            answer_tokens = inputs['input_ids'][0][start_idx:end_idx+1]\n",
        "            answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
        "        else:\n",
        "            answer = \"No answer found\"\n",
        "\n",
        "        return answer, start_confidence, end_confidence\n",
        "\n",
        "# Test with complex examples\n",
        "test_examples = [\n",
        "    {\n",
        "        \"context\": \"The Amazon rainforest, also known as Amazonia, is a moist broadleaf tropical rainforest in the Amazon biome that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 km2, of which 5,500,000 km2 are covered by the rainforest. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana.\",\n",
        "        \"question\": \"Which country contains the most Amazon rainforest?\"\n",
        "    },\n",
        "    {\n",
        "        \"context\": \"Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.\",\n",
        "        \"question\": \"What can machine learning systems do with minimal human intervention?\"\n",
        "    },\n",
        "    {\n",
        "        \"context\": \"The Great Wall of China is a series of fortifications made of stone, brick, tamped earth, wood, and other materials, generally built along an east-to-west line across the historical northern borders of China.\",\n",
        "        \"question\": \"What materials were used to build the Great Wall?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"\\nTesting LoRA-tuned QA model:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, example in enumerate(test_examples, 1):\n",
        "    answer, start_conf, end_conf = answer_question(\n",
        "        example[\"question\"],\n",
        "        example[\"context\"],\n",
        "        model,\n",
        "        tokenizer\n",
        "    )\n",
        "\n",
        "    print(f\"Example {i}:\")\n",
        "    print(f\"Question: {example['question']}\")\n",
        "    print(f\"Answer: {answer}\")\n",
        "    print(f\"Confidence: Start={start_conf:.3f}, End={end_conf:.3f}\")\n",
        "    print(\"-\" * 60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "id": "JPnMyIZMV6kF",
        "outputId": "95571bff-1a39-4a5c-a16b-02fe48a6a107"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating LoRA-tuned model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='127' max='127' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [127/127 00:04]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Evaluation Results:\n",
            "----------------------------------------\n",
            "Loss: 1.6752\n",
            "Start_Accuracy: 0.5044\n",
            "End_Accuracy: 0.4975\n",
            "Exact_Match: 0.3872\n",
            "Runtime: 5.1360\n",
            "Samples_Per_Second: 197.6260\n",
            "Steps_Per_Second: 24.7280\n",
            "\n",
            "Testing LoRA-tuned QA model:\n",
            "============================================================\n",
            "Example 1:\n",
            "Question: Which country contains the most Amazon rainforest?\n",
            "Answer: the\n",
            "Confidence: Start=0.434, End=0.514\n",
            "------------------------------------------------------------\n",
            "Example 2:\n",
            "Question: What can machine learning systems do with minimal human intervention?\n",
            "Answer: machine\n",
            "Confidence: Start=0.713, End=0.546\n",
            "------------------------------------------------------------\n",
            "Example 3:\n",
            "Question: What materials were used to build the Great Wall?\n",
            "Answer: the\n",
            "Confidence: Start=0.429, End=0.326\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance Analysis and Comparison"
      ],
      "metadata": {
        "id": "xeGu48w1V_Xy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory efficiency analysis\n",
        "def analyze_lora_efficiency():\n",
        "    \"\"\"Compare LoRA vs full fine-tuning efficiency\"\"\"\n",
        "    base_params = sum(p.numel() for p in model.base_model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(\"LoRA Efficiency Analysis:\")\n",
        "    print(f\"Base model parameters: {base_params:,}\")\n",
        "    print(f\"LoRA trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"Parameter reduction: {(1 - trainable_params/base_params)*100:.2f}%\")\n",
        "    print(f\"Memory savings: ~{(base_params - trainable_params) * 4 / 1024**2:.1f} MB\")\n",
        "\n",
        "analyze_lora_efficiency()\n",
        "\n",
        "# Advanced debugging for QA\n",
        "def debug_qa_prediction(question, context, model, tokenizer):\n",
        "    \"\"\"Debug QA model predictions with attention analysis\"\"\"\n",
        "    inputs = tokenizer(\n",
        "        question,\n",
        "        context,\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        return_tensors=\"pt\",\n",
        "        return_attention_mask=True\n",
        "    ).to(device)\n",
        "\n",
        "    # Get tokens for visualization\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_attentions=True)\n",
        "        start_scores = outputs.start_logits[0]\n",
        "        end_scores = outputs.end_logits[0]\n",
        "\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Context length: {len(context)} characters\")\n",
        "    print(f\"Tokenized length: {len(tokens)} tokens\")\n",
        "    print(f\"Top 3 start positions: {torch.topk(start_scores, 3).indices.tolist()}\")\n",
        "    print(f\"Top 3 end positions: {torch.topk(end_scores, 3).indices.tolist()}\")\n",
        "\n",
        "# Debug a challenging example\n",
        "debug_qa_prediction(\n",
        "    \"What is the percentage of Amazon rainforest in Brazil?\",\n",
        "    test_examples[0][\"context\"],\n",
        "    model,\n",
        "    tokenizer\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9gMp8O5V_QF",
        "outputId": "ae97b5ce-4af6-417b-b839-6e038bed9473"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA Efficiency Analysis:\n",
            "Base model parameters: 66,955,780\n",
            "LoRA trainable parameters: 591,362\n",
            "Parameter reduction: 99.12%\n",
            "Memory savings: ~253.2 MB\n",
            "Question: What is the percentage of Amazon rainforest in Brazil?\n",
            "Context length: 474 characters\n",
            "Tokenized length: 119 tokens\n",
            "Top 3 start positions: [12, 82, 92]\n",
            "Top 3 end positions: [12, 98, 83]\n"
          ]
        }
      ]
    }
  ]
}